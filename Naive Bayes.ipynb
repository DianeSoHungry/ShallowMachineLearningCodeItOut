{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is inspired but not limited by *Machine Learning In Action*.\n",
    "e.g., the implementation of the algrithm is at a higher level.\n",
    "\n",
    "All rights deserved by Diane(Qingyun Hu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. About Naive Bayes\n",
    "\n",
    "## 1.1 Mechanism of Naive Bayes\n",
    "Naive Bayes is a variant of Bayes' Rule. Let's recap Bayes' Rule a bit.\n",
    "\n",
    "$$ P(c_i | w_1, w_2, w_3, ..., w_m) = \\frac{P(w_1, w_2, w_3, ..., w_m | c_i)*P(c_i)}{P(w_1, w_2, w_3, ..., w_m)} $$\n",
    "\n",
    "where $w_1, w_2, w_3, ..., w_m$ is an vector of words that present in the document as well as included in the existing vocabulary list, and $c_i$ stands for class i.\n",
    "\n",
    "Naive Bayes asks us to assume that the presence of $w_1, w_2, w_3, ..., w_m$ is independent. Although this is not realistic, as in there are always some connection between one word to another. However, this assumption simplifies the calculation and works quite well so far. By assuming the presence of words is independent, here we have:\n",
    "\n",
    "$$ P(c_i | w_1, w_2, w_3, ..., w_m) = \\frac{(\\ P(w_1 | c_i) * P(w_2 | c_i) * P(w_3 | c_i) * ... * P(w_m | c_i)\\ ) * P(c_i)}{P(w_1) * P(w_2) * P(w_3) * ... * P(w_m))} $$\n",
    "\n",
    "\n",
    "\n",
    "## 1.2 Pros and Cons\n",
    "### 1.21 Pros\n",
    "1. Handles multiple classes.\n",
    "2. Works well on small dataset.\n",
    "\n",
    "### 1.22 Cons\n",
    "1. Sensitive to how the input data is prepared\n",
    "2. The sparse bag-of-words vector could consume a lot of memery if not handling it properly, as in for each vector, it's lenth is at the same lenth of vocabulary list.\n",
    "\n",
    "### 1.23 Works with\n",
    "Nominal Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ID3 Tree Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat demo dataset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import math\n",
    "def createDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', \\\n",
    "                  'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', \\\n",
    "                  'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', \\\n",
    "                   'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how',\\\n",
    "                   'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "dataSet, labels = createDataSet()\n",
    "dataSet\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tool Function 1: Create an vocabulary list according to dataSet\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabList = set([])\n",
    "    for docum in dataSet:\n",
    "        vocabList = vocabList | set(docum)\n",
    "    return list(vocabList)\n",
    "vocabList = createVocabList(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,\n",
       "        1.,  1.,  2.,  1.,  1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  2.,  1.,  1.,  2.,  1.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool Function 2: Get an bag of words vector for each document\n",
    "import numpy as np\n",
    "def bagOfWordsVec(vocabList, document):\n",
    "    returnVec = np.ones(len(vocabList))\n",
    "    for token in document:\n",
    "        if token in vocabList:\n",
    "            returnVec[vocabList.index(token)] += 1\n",
    "    return returnVec\n",
    "bagOfWordsVec(vocabList, dataSet[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>park</th>\n",
       "      <th>food</th>\n",
       "      <th>licks</th>\n",
       "      <th>him</th>\n",
       "      <th>problems</th>\n",
       "      <th>love</th>\n",
       "      <th>take</th>\n",
       "      <th>not</th>\n",
       "      <th>maybe</th>\n",
       "      <th>mr</th>\n",
       "      <th>...</th>\n",
       "      <th>has</th>\n",
       "      <th>so</th>\n",
       "      <th>I</th>\n",
       "      <th>help</th>\n",
       "      <th>stupid</th>\n",
       "      <th>dalmation</th>\n",
       "      <th>ate</th>\n",
       "      <th>worthless</th>\n",
       "      <th>my</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   park  food  licks  him  problems  love  take  not  maybe   mr  ...    has  \\\n",
       "0   1.0   1.0    1.0  1.0       2.0   1.0   1.0  1.0    1.0  1.0  ...    2.0   \n",
       "1   2.0   1.0    1.0  2.0       1.0   1.0   2.0  2.0    2.0  1.0  ...    1.0   \n",
       "2   1.0   1.0    1.0  2.0       1.0   2.0   1.0  1.0    1.0  1.0  ...    1.0   \n",
       "3   1.0   1.0    1.0  1.0       1.0   1.0   1.0  1.0    1.0  1.0  ...    1.0   \n",
       "4   1.0   1.0    2.0  2.0       1.0   1.0   1.0  1.0    1.0  2.0  ...    1.0   \n",
       "5   1.0   2.0    1.0  1.0       1.0   1.0   1.0  1.0    1.0  1.0  ...    1.0   \n",
       "\n",
       "    so    I  help  stupid  dalmation  ate  worthless   my  label  \n",
       "0  1.0  1.0   2.0     1.0        1.0  1.0        1.0  2.0      0  \n",
       "1  1.0  1.0   1.0     2.0        1.0  1.0        1.0  1.0      1  \n",
       "2  2.0  2.0   1.0     1.0        2.0  1.0        1.0  2.0      0  \n",
       "3  1.0  1.0   1.0     2.0        1.0  1.0        2.0  1.0      1  \n",
       "4  1.0  1.0   1.0     1.0        1.0  2.0        1.0  2.0      0  \n",
       "5  1.0  1.0   1.0     2.0        1.0  1.0        2.0  1.0      1  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool Function 3: Get BagOfWordsTable for Training Dataset\n",
    "\n",
    "def getBagOfWordsTable(dataSet, vocabList, label):\n",
    "    bagOfWordsTable = []\n",
    "    for document in dataSet:\n",
    "        bagOfWordsTable.append(bagOfWordsVec(vocabList, document))\n",
    "    bagOfWordsTable = pd.DataFrame(bagOfWordsTable, columns=vocabList)\n",
    "    bagOfWordsTable['label']= label\n",
    "    return bagOfWordsTable\n",
    "getBagOfWordsTable(dataSet, vocabList, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Traing DataSet (The propability of each document being Class 1) :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18178454867713456,\n",
       " 1.2017140246697071,\n",
       " 0.12570863705130642,\n",
       " 1.1353438671320581,\n",
       " 0.14790295856460187,\n",
       " 1.4539243496229779]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Classes of Traing DataSet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Bad!\n"
     ]
    }
   ],
   "source": [
    "# Calculate Probabilities\n",
    "\n",
    "bagOfWordsTable = getBagOfWordsTable(dataSet, vocabList, labels)\n",
    "def getProb(c_i, bagOfWordsTable, testDataset):\n",
    "    P_ci = bagOfWordsTable['label'][bagOfWordsTable.label==c_i].count() / bagOfWordsTable.shape[0]\n",
    "    bagOfWordsTable_ci = bagOfWordsTable[bagOfWordsTable.label==c_i]\n",
    "    P_Xi_ci = bagOfWordsTable_ci.sum() / bagOfWordsTable_ci.sum().sum()\n",
    "    P_Xi = bagOfWordsTable.sum() / bagOfWordsTable.sum().sum()\n",
    "    \n",
    "    predVec = []\n",
    "    for document in testDataset:\n",
    "        predVec.append(np.exp(np.log(P_Xi_ci[document]).sum() + np.log(P_ci) - np.log(P_Xi[document]).sum()))\n",
    "#     return P_Xi_ci, P_ci, P_Xi\n",
    "    return predVec\n",
    "\n",
    "print(\"Predictions on Traing DataSet (The propability of each document being Class 1) :\")\n",
    "getProb(1, bagOfWordsTable,dataSet)\n",
    "\n",
    "print(\"Real Classes of Traing DataSet\")\n",
    "labels\n",
    "\n",
    "print(\"Not Bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3. Misc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trick 1: Initiate bag-of-words vector with 1s instead of 0s to prevent something like $P(w_i|c_1)==0$ from happening, which would cause the prediction to be 0.\n",
    "\n",
    "Trick 2: Probability varies from 0 to 1, thus when multiplying a bunch of probabilities like $P(w_1) * P(w_2) * P(w_3) * ... * P(w_m)$, underflow tend to happen. To prevent this from happening, what we can do is to apply log() first then exp() to the right side of the equation $P(c_i | w_1, w_2, w_3, ..., w_m) = \\frac{(\\ P(w_1 | c_i) * P(w_2 | c_i) * P(w_3 | c_i) * ... * P(w_m | c_i)\\ ) * P(c_i)}{P(w_1) * P(w_2) * P(w_3) * ... * P(w_m))} $ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
